{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RZ-UctGr64u5",
        "outputId": "b1d30ed5-2dde-4084-8c14-fc13bb2fc0b2"
      },
      "outputs": [],
      "source": [
        "#@title Install Gurobi\n",
        "!pip install gurobipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9U6iQOGnF8m"
      },
      "source": [
        "# Implementation with Multiprocessing (Process + Queue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDPNU8Umoey8"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0JrOc5QnP25",
        "outputId": "1cc721f7-284f-4b11-d259-6604868cfafe"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import multiprocessing\n",
        "import time\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from gurobipy import GRB, Model\n",
        "from collections import deque\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import drive\n",
        "import operator\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qdC7sVAnQew"
      },
      "outputs": [],
      "source": [
        "### Helper functions\n",
        "\n",
        "# Perform topological sort\n",
        "# Returns a list of node indices in topological order. If a cycle is detected, returns an empty list.\n",
        "def topological_sort(graph):\n",
        "    in_degree = {u: 0 for u in graph}\n",
        "    for u in graph:\n",
        "        for v in graph[u]:\n",
        "            in_degree[v] += 1\n",
        "\n",
        "    queue = deque([u for u in in_degree if in_degree[u] == 0])\n",
        "    top_order = []\n",
        "\n",
        "    while queue:\n",
        "        u = queue.popleft()\n",
        "        top_order.append(u)\n",
        "        for v in graph[u]:\n",
        "            in_degree[v] -= 1\n",
        "            if in_degree[v] == 0:\n",
        "                queue.append(v)\n",
        "\n",
        "    if len(top_order) != len(in_degree):\n",
        "        return []  # Cycle detected, not a valid ordering\n",
        "    return top_order\n",
        "\n",
        "# Get the ordering of products based on the sign representation of a polyhedron\n",
        "# returns the topological sort (linear ordering) of the products\n",
        "def get_product_ordering(n, poly_rep):\n",
        "    \"\"\"\n",
        "    n: Number of products\n",
        "    poly_rep: A sign vector (+1, -1) defining the orientation of each hyperplane.\n",
        "\n",
        "    returns a list, which is the topological ordering of the products based on the polyhedron representation.\n",
        "    \"\"\"\n",
        "    graph = {i: [] for i in range(n)}\n",
        "    index = 0\n",
        "    for s, t in combinations(range(n), 2):\n",
        "        direction = poly_rep[index]\n",
        "        if direction == 1:\n",
        "            graph[s].append(t)\n",
        "        elif direction == -1:\n",
        "            graph[t].append(s)\n",
        "        index += 1\n",
        "    return topological_sort(graph)\n",
        "\n",
        "# Checks if a polyhedron, defined by poly_rep, is full-dimensional using a linear program (LP).\n",
        "def full_dimensionality_test(model, poly_rep, a_array, m):\n",
        "    \"\"\"\n",
        "    model: Gurobi model object\n",
        "    poly_rep: A sign vector (+1, -1) defining the orientation of each hyperplane.\n",
        "    a_array: The array of normal vectors for all hyperplanes.\n",
        "    m: Number of hyperplanes.\n",
        "    \"\"\"\n",
        "    is_full_dimensional = True\n",
        "    # -diag(poly_rep) @ a_array @ x + epsilon <= 0\n",
        "    A_coeff = -np.diag(poly_rep) @ a_array                    # Create the coefficient matrix for the constraints, each constraint corresponds to a hyperplane.\n",
        "    A_coeff_full = np.hstack([A_coeff, np.ones((m, 1))])\n",
        "    model_copy = model.copy()\n",
        "    vars_array = model_copy.getVars()\n",
        "    model_copy.addMConstr(A_coeff_full, vars_array, GRB.LESS_EQUAL, np.zeros(m))\n",
        "    model_copy.update()\n",
        "    model_copy.optimize()\n",
        "\n",
        "    # The polyhedron is not full-dimensional if the LP is infeasible or the optimal objective is 0.\n",
        "    if model_copy.status == GRB.INFEASIBLE or (model_copy.status == GRB.OPTIMAL and model_copy.ObjVal == 0):\n",
        "        is_full_dimensional = False\n",
        "\n",
        "    # returns True if the polyhedron is full-dimensional, False otherwise.\n",
        "    return is_full_dimensional\n",
        "\n",
        "# Generates the sign vector for a neighboring polyhedron by flipping the sign corresponding to the hyperplane between two adjacent items in the ordering.\n",
        "def get_new_poly_rep(poly_rep, ordering, first_pos, a_array_index):\n",
        "    \"\"\"\n",
        "    poly_rep (array): A sign vector (+1, -1) defining the orientation of each hyperplane.\n",
        "    ordering (list): The current ordering of products.\n",
        "    first_pos: The index of the first product in the adjacent pair to be swapped.\n",
        "    a_array_index: A mapping from a product pair (i, j) to its hyperplane index.\n",
        "\n",
        "    returns the new sign vector\n",
        "    \"\"\"\n",
        "    element1, element2 = ordering[first_pos], ordering[first_pos + 1]\n",
        "    rep_index = a_array_index[(element1, element2)]\n",
        "    new_poly_rep = poly_rep.copy()\n",
        "    new_poly_rep[rep_index] = -new_poly_rep[rep_index]\n",
        "    return new_poly_rep\n",
        "\n",
        "# Generates a new product ordering by swapping two adjacent elements.\n",
        "def get_new_ordering(ordering, first_pos):\n",
        "    if 0 <= first_pos <= len(ordering) - 2:\n",
        "        new_ordering = ordering.copy()\n",
        "        new_ordering[first_pos], new_ordering[first_pos + 1] = new_ordering[first_pos + 1], new_ordering[first_pos]\n",
        "        return new_ordering\n",
        "    else:\n",
        "        print(\"Invalid index\")\n",
        "\n",
        "# Converts a polyhedron sign vector (e.g., [1, -1, 1, 1]) into a compact integer bitmask.\n",
        "def polyhedron_to_bitmask(arr: np.ndarray) -> int:\n",
        "    bitmask = 0\n",
        "    for idx, val in enumerate(arr):\n",
        "        if val == -1:\n",
        "            bitmask |= 1 << idx  # Set bit at position idx if value is -1\n",
        "    return bitmask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnZyiLypSz20"
      },
      "outputs": [],
      "source": [
        "# Sets up a complete problem instance with n products.\n",
        "# Includes generating random revenues, costs, and attractiveness values, and defining the multi-objective pseudo-revenue vectors and hyperplanes.\n",
        "def initialize_parameters(n, rev_range, c_range, v_range, seed):\n",
        " # Core parameters\n",
        "    params = {}\n",
        "    params['n'] = n\n",
        "    params['m'] = int(n*(n-1)/2)  # Number of hyperplanes = C(n,2)\n",
        "    params['d'] = 3               # Dimension of pseudo-revenue vectors (number of objectives)\n",
        "    params['half_n'] = n // 2     # Half the products for cost splitting\n",
        "\n",
        "    # Generate random parameters\n",
        "    np.random.seed(seed)\n",
        "    params['rev_Init'] = np.random.uniform(*rev_range, n)   # Raw revenue vector\n",
        "    params['cost_Init'] = np.random.uniform(*c_range, n)        # Raw cost vector\n",
        "    params['v_Init'] = np.random.uniform(*v_range, n)     # Raw attractiveness vector\n",
        "\n",
        "\n",
        "    # Sort by descending revenue\n",
        "    sort_idx = params['rev_Init'].argsort()[::-1]  # Descending order indices\n",
        "    params['rev'] = params['rev_Init'][sort_idx]   # Sorted revenue vector\n",
        "    params['cost'] = params['cost_Init'][sort_idx] # Aligned cost vector\n",
        "    params['v'] = params['v_Init'][sort_idx]       # Aligned attractiveness vector\n",
        "    params['sort_indices'] = sort_idx              # Preserve sorting indices\n",
        "\n",
        "    # Split costs into two groups\n",
        "    permuted_idx = np.random.permutation(n)\n",
        "    params['group1_idx'] = permuted_idx[:params['half_n']]  # First group indices\n",
        "    params['group2_idx'] = permuted_idx[params['half_n']:]  # Second group indices\n",
        "\n",
        "    # Initialize cost arrays with original positions maintained\n",
        "    params['cost1'] = np.zeros_like(params['cost'])\n",
        "    params['cost2'] = np.zeros_like(params['cost'])\n",
        "    params['cost1'][params['group1_idx']] = params['cost'][params['group1_idx']]\n",
        "    params['cost2'][params['group2_idx']] = params['cost'][params['group2_idx']]\n",
        "\n",
        "# Create pseudo-revenue matrix\n",
        "    # Each row: [revenue, cost1, cost2] for a product\n",
        "    params['coeff'] = np.column_stack((params['rev'], params['cost1'], params['cost2']))\n",
        "\n",
        "#Generate hyperplane\n",
        "    params['a_array'] = []\n",
        "    params['a_array_index'] = {}\n",
        "\n",
        "    # Create hyperplane from all product pairs\n",
        "    for idx, (i,j) in enumerate(combinations(range(n), 2)):\n",
        "        # Hyperplane normal = difference between pseudo-revenue vectors\n",
        "        params['a_array'].append(params['coeff'][i] - params['coeff'][j])\n",
        "\n",
        "        #Create bidirectional index mapping (i,j) <-> (j,i)\n",
        "        params['a_array_index'][(i,j)] = idx\n",
        "        params['a_array_index'][(j,i)] = idx\n",
        "\n",
        "    params['a_array'] = np.array(params['a_array'])  # Convert to numpy array\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa_PokzFx-rj"
      },
      "outputs": [],
      "source": [
        "# worker function for each parallel process.\n",
        "def consumer(state_queue, visited_set, candidate_assortments, task_counter, num_consumers, a_array, a_array_index, m, model, full_d_count):\n",
        "    while True:\n",
        "        item = state_queue.get()\n",
        "        if item==-1:\n",
        "          \"Terminate here.\"\n",
        "          break\n",
        "\n",
        "        poly_rep, ordering = item\n",
        "        n = len(ordering)\n",
        "        # Explore neighbors by swapping every adjacent pair in the current ordering.\n",
        "        for first_pos in range(n - 1):\n",
        "            neighbor_rep = get_new_poly_rep(poly_rep, ordering, first_pos, a_array_index)\n",
        "            neighbor_rep_mask = polyhedron_to_bitmask(neighbor_rep)\n",
        "\n",
        "            # Check if this neighbor has already been visited to avoid redundant work.\n",
        "            if neighbor_rep_mask not in visited_set:\n",
        "                visited_set[neighbor_rep_mask] = True\n",
        "                test = full_dimensionality_test(model, neighbor_rep, a_array, m)  # Perform the expensive full-dimensionality test.\n",
        "                if test:                                                          # If it's a valid new state, add it to the queue for further exploration.\n",
        "                    new_ordering = get_new_ordering(ordering, first_pos)\n",
        "                    state_queue.put((neighbor_rep, new_ordering))\n",
        "                    with task_counter.get_lock():\n",
        "                        task_counter.value += 1\n",
        "                    candidate_assortments.append(frozenset(new_ordering[:first_pos + 1]))   # From the new ordering, generate a candidate assortment.\n",
        "                    with full_d_count.get_lock():\n",
        "                      full_d_count.value += 1\n",
        "\n",
        "        # all neighbors are checked, decrement the counter.\n",
        "        with task_counter.get_lock():\n",
        "            task_counter.value -= 1\n",
        "            # If the counter reaches 0, it means the entire state space has been explored.\n",
        "            # send termination signals to all other workers.\n",
        "            if task_counter.value == 0:\n",
        "              for _ in range(num_consumers):\n",
        "                state_queue.put(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh-r6NXAAkkE"
      },
      "outputs": [],
      "source": [
        "# Run the entire parallel breadth-first search of the polyhedron state space.\n",
        "def run_experiment_multiprocessing(initial_points, num_consumers, a_array, a_array_index, n, m):\n",
        "    \"\"\"\n",
        "    initial_points: List of initial points in the polyhedron state space.\n",
        "    num_consumers: Number of parallel processes to run.\n",
        "    a_array: Array of hyperplane normal vectors.\n",
        "    a_array_index: Mapping from product pairs to hyperplane indices.\n",
        "    n: Number of products.\n",
        "    m: Number of hyperplanes.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    with multiprocessing.Manager() as manager:\n",
        "        state_queue = multiprocessing.Queue()\n",
        "        visited_set = manager.dict()                      #  Shared dictionary to track visited states.\n",
        "        candidate_assortments = manager.list()            # Shared list to store candidate assortments.\n",
        "        full_d_count = multiprocessing.Value('i', 0)      # Global counter for full-dimensional polyhedra found.\n",
        "        task_counter = multiprocessing.Value('i', 0)      # Global counter for active tasks in the queue.\n",
        "\n",
        "        model = Model(\"LP\")\n",
        "        model.setParam(\"OutputFlag\", 0)\n",
        "        model.setParam('Presolve', 1)\n",
        "        x = model.addMVar(3, lb=-GRB.INFINITY, ub=GRB.INFINITY, name=\"x\")\n",
        "        epsilon = model.addVar(lb=-GRB.INFINITY, ub=1, name=\"epsilon\")\n",
        "        model.setObjective(epsilon, GRB.MAXIMIZE)\n",
        "        model.update()\n",
        "\n",
        "        for point in initial_points:\n",
        "            # Initialize the polyhedra corresponding to initial points, compute the ordering of products in it and convert it to bitmask.\n",
        "            poly_rep = np.sign(np.dot(a_array, point))        \n",
        "            ordering = get_product_ordering(n,poly_rep)\n",
        "            bit_mask = polyhedron_to_bitmask(poly_rep)\n",
        "\n",
        "            # if the focal polyhedron has not been visited yet, add it to the queue and mark it as visited.\n",
        "            if bit_mask not in visited_set:\n",
        "                visited_set[bit_mask] = True\n",
        "                state_queue.put((poly_rep, ordering))\n",
        "                with task_counter.get_lock():\n",
        "                    task_counter.value += 1\n",
        "            # Generate initial candidate assortments from the first ordering.\n",
        "            current_elements = set()          # If not visited, take first k (k=1...n) products as an assortment and add to the candidates\n",
        "            for element in ordering:\n",
        "                current_elements.add(element)\n",
        "                candidate_assortments.append(frozenset(current_elements))\n",
        "\n",
        "        processes = []\n",
        "        for i in range(num_consumers):\n",
        "            p = multiprocessing.Process(\n",
        "                target=consumer,\n",
        "                args=(state_queue, visited_set, candidate_assortments, task_counter, num_consumers, a_array, a_array_index, m, model, full_d_count)\n",
        "            )\n",
        "            processes.append(p)   \n",
        "            p.start()\n",
        "        # Wait for all consumer processes to complete their work and terminate.\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "\n",
        "      # Retrieve the final results\n",
        "        visited_set_final = dict(visited_set)  # Copy shared dictionary to local\n",
        "        candidate_assortments_final = set(candidate_assortments)  # Copy shared list to local\n",
        "\n",
        "        end_time = time.time()\n",
        "        SPBFS_total_time = end_time - start_time\n",
        "        return {\n",
        "          \"visited_set\": visited_set_final,\n",
        "          \"candidate_assortments\": candidate_assortments_final,\n",
        "          \"full_d_count\": full_d_count.value,\n",
        "          \"product #\": n,\n",
        "          \"visited_P_#\": len(visited_set),\n",
        "          \"candidate_assortments_#\": len(candidate_assortments_final),\n",
        "          \"SPBFS_Total_time\": SPBFS_total_time                           # LP_total_time + Initialization_total_time = Total time of SPBFS\n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q9YJTrBCNSX"
      },
      "outputs": [],
      "source": [
        "def dict_to_vertical_df(d):\n",
        "    \"\"\"\n",
        "    Converts a dictionary `d` into a DataFrame where:\n",
        "      - Each key in `d` becomes a DataFrame column.\n",
        "      - If the value is scalar, it is repeated on every row.\n",
        "      - If the value is a list or ndarray, its elements are placed row by row.\n",
        "    The number of rows = max length among all list/array values.\n",
        "    Shorter lists or scalars get None in the extra rows.\n",
        "\n",
        "    Returns a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # 1) Determine how many rows we need\n",
        "    max_len = 1\n",
        "    for v in d.values():\n",
        "        if isinstance(v, (list, np.ndarray)):\n",
        "            max_len = max(max_len, len(v))\n",
        "\n",
        "    # 2) Create a data dict with columns for each key,\n",
        "    #    all initialized to None\n",
        "    data = {k: [None]*max_len for k in d.keys()}\n",
        "\n",
        "    # 3) Fill column by column\n",
        "    for key, val in d.items():\n",
        "        if isinstance(val, (list, np.ndarray)):\n",
        "            # Place each element vertically in the column\n",
        "            for i, element in enumerate(val):\n",
        "                data[key][i] = element\n",
        "        else:\n",
        "            # It's scalar, so repeat the same value in every row\n",
        "            for i in range(max_len):\n",
        "                data[key][i] = val\n",
        "\n",
        "    # 4) Build the DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "def expand_candidate_assortment(results, assortment_key=\"candidate_assortments\"):\n",
        "    # 1) Separate out the candidate assortment\n",
        "    candidate_assortment = results.get(assortment_key, frozenset())\n",
        "\n",
        "    # 2) Copy everything else into 'common_fields'\n",
        "    #    so we don't lose it\n",
        "    common_fields = {\n",
        "        k: v for k, v in results.items() if k != assortment_key\n",
        "    }\n",
        "\n",
        "    # 3) Build a list of row dicts\n",
        "    rows = []\n",
        "    for sub_fs in candidate_assortment:\n",
        "        # Each 'sub_fs' is one of the frozensets in the parent frozenset\n",
        "        row = dict(common_fields)\n",
        "        # Convert the sub-frozenset to a list (or you can keep it as string)\n",
        "        row[assortment_key] = list(sub_fs)\n",
        "        rows.append(row)\n",
        "\n",
        "    # 4) Convert to a DataFrame\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIVFm9fFom6R"
      },
      "source": [
        "## Runing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKzxdcKyAknA",
        "outputId": "18c02057-23a4-46d1-bbd8-c41b16ae642f"
      },
      "outputs": [],
      "source": [
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    google_drive_dir = \"/content/drive/MyDrive/my_experiments\"\n",
        "\n",
        "    n = 10\n",
        "    num_initial_points = 16\n",
        "    num_consumers = multiprocessing.cpu_count()\n",
        "    rev_range = (0.5, 1)                # Revenue value range\n",
        "    c_range = (0.5, 1)                  # Cost value range\n",
        "    v_range = (0, 1)\n",
        "\n",
        "    total_runs = 50                     # total number of independent experiments to run for the given n, each run uses a different random seed\n",
        "    runs_per_group = 50                 # how many individual runs are in a single group\n",
        "                                        # reduce the number and run with different `group_index` below if having timeout issues in Google Colab\n",
        "    total_groups = total_runs // runs_per_group  # total number of groups to run for the given n, each group runs in parallel\n",
        "    all_seeds = list(range(total_runs))\n",
        "    \n",
        "    # which slice of the group to run, MANUALLY CHANGE THIS VALUE TO RUN DIFFERENT GROUPS (to avoid timeout issues)\n",
        "    group_index = 0              # 0,1,2,3,4,5,6,7,8,9\n",
        "\n",
        "    # use the group_index to select the specific slice of seeds for the current execution\n",
        "    start = group_index * runs_per_group\n",
        "    end = start + runs_per_group\n",
        "    group_seeds = all_seeds[start:end]\n",
        "\n",
        "    for run_seed in group_seeds:\n",
        "      # Initialize all parameters for the current run using its unique seed.\n",
        "      params = initialize_parameters(n, rev_range, c_range, v_range, seed = run_seed)        # use the index of run as random seed\n",
        "      a_array, a_array_index, m = params['a_array'], params['a_array_index'], params['m']\n",
        "\n",
        "      #Generate random starting points and run the parallel search algorithm\n",
        "      initial_points = [np.random.randn(3)*10 for _ in range(num_initial_points)]\n",
        "      results = run_experiment_multiprocessing(initial_points, num_consumers, a_array, a_array_index, n, m)\n",
        "\n",
        "      # Save and organize parameters and results\n",
        "      keys_to_extract_params = ['rev', 'cost', 'v', 'group1_idx', 'group2_idx', 'cost1', 'cost2']\n",
        "      getter_params = operator.itemgetter(*keys_to_extract_params)\n",
        "      vals_params = getter_params(params)\n",
        "      params_record = dict(zip(keys_to_extract_params, vals_params))\n",
        "\n",
        "      # Convert the records into DataFrames formatted for CSV export.\n",
        "      keys_to_extract_results = ['candidate_assortments', 'full_d_count', 'product #', 'visited_P_#', 'candidate_assortments_#', 'SPBFS_Total_time']\n",
        "      getter_results = operator.itemgetter(*keys_to_extract_results)\n",
        "      vals_results = getter_results(results)\n",
        "      results_record = dict(zip(keys_to_extract_results, vals_results))\n",
        "\n",
        "      df_params = dict_to_vertical_df(params_record)\n",
        "      df_results = expand_candidate_assortment(results_record)\n",
        "\n",
        "      params_filename = f\"{google_drive_dir}/n={n}_seed_{run_seed}_params.csv\"\n",
        "      results_filename = f\"{google_drive_dir}/n={n}_seed_{run_seed}_results.csv\"\n",
        "      df_results.to_csv(results_filename, index=False)\n",
        "      df_params.to_csv(params_filename, index=False)\n",
        "\n",
        "    print(\"Done. Group\", group_index, \"saved to Google Drive.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
